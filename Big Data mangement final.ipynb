{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd6940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from hdfs import InsecureClient\n",
    "import os\n",
    "import re\n",
    "from hdfs.ext.kerberos import KerberosClient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbcfa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark  = SparkSession.builder.master(\"local[1]\").appName(\"importSequence1\")\\\n",
    "    .config(\"spark.driver.memory\",\"4g\")\\\n",
    "    .config(\"spark.executor.memory\",\"4g\")\\\n",
    "    .config(\"spark.driver.maxResultSize\",\"2g\")\\\n",
    "    .getOrCreate()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed0b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=spark.sparkContext.textFile(\"/Users/brzoskpm/blast_databases/hg38.fa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db2a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence=\"\".join(rdd.collect()[1:len(rdd.collect())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd1ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instead of creating list out of rdd, just use collect in the lamda so that I don;t have to convert back into rdd \n",
    "#using parrlelize which mamkes it parralelizerdd isntead of mappartition rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea9d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build kmers\n",
    "def build_kmers(sequence, ksize):\n",
    "    kmers = pd.DataFrame()\n",
    "    kmerList = []\n",
    "    indexList=[]\n",
    "    n_kmers = len(sequence) - ksize + 1\n",
    "    #f = open(\"/Users/karlbrzoska/Documents/BigDataManagementProject/kmers.txt\", \"w\")\n",
    "    for i in range(n_kmers):\n",
    "        kmerList.append(sequence[i:i + ksize])\n",
    "        indexList.append(i)\n",
    "\n",
    "  \n",
    "     \n",
    "    #kmers['index']=indexList\n",
    "    kmers['kmers']=kmerList\n",
    "    \n",
    "        \n",
    "    return kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6a3ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kmersrdd(rdd):\n",
    "    list_of_rdd_kmers =[]\n",
    "    sequence_list= rdd.collect()\n",
    "    for i in sequence_list:\n",
    "        list_of_rdd_kmers.append(spark.sparkContext.parallelize(build_kmers(i, 100)))\n",
    "    return list_of_rdd_kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8393bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmers=build_kmers(sequence, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96158f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDFSPublisher:\n",
    "\n",
    "  # df is the datasource we wish to write to HDFS\n",
    "  # filename is what we want the name of the file to be on HDFS\n",
    "  # this function will pass through to the HDFS client write()\n",
    "\n",
    "  def write(self, df: pd.DataFrame, filename: str) -> object:\n",
    "    \n",
    "    host = 'http://localhost:9870'\n",
    "    my_client = KerberosClient(host)\n",
    "\n",
    "    # remember we received the DataFrame and the filename as parameters\n",
    "    with my_client.write(filename, encoding='utf-8') as writer:\n",
    "      df.to_csv(writer, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eae3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "publisher = HDFSPublisher()\n",
    "publisher.write(kmers, 'kmersTest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe7ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('hdfs://localhost:9000/user/dr.who/kmers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e2b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('_c1').count().write.csv('hdfs://localhost:9000/user/dr.who/kmersCount.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3677c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InsecureClient('http://localhost:9870/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53467d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.list('/user/dr.who')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7aca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split file\n",
    "with open(\"/Users/brzoskpm/Karl/genomes/hg38.fa\",'r') as data_file:\n",
    "    out = open(\"dummy\", \"w\")\n",
    "    for line in data_file:\n",
    "        x = re.search(\"^>\", line)\n",
    "        if (x):\n",
    "            out.close()\n",
    "            #try: out.close()\n",
    "            tmp=line.split()\n",
    "            name = tmp[0].replace(\">\",\"\")\n",
    "            out = open(name, \"w\")\n",
    "            out.write(line)\n",
    "            print(line)\n",
    "        else:\n",
    "            out.write(line)\n",
    "            print(line)\n",
    "        \n",
    "out.close()\n",
    "        \n",
    "        #data = line.split('>')\n",
    "        #print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "import subprocess\n",
    "import os\n",
    "for filename in os.listdir(\"/Users/brzoskpm/Karl/genomes/\"):\n",
    "    print(filename)\n",
    "    rdd=spark.sparkContext.textFile(\"file:///Users/brzoskpm/Karl/genomes/\"+filename)\n",
    "    print(\"rdd finished\")\n",
    "    sequence=\"\".join(rdd.collect()[1:len(rdd.collect())])\n",
    "    print(\"sequence finished\")\n",
    "    #list=[]\n",
    "    #list.append(sequence)\n",
    "    kmers=build_kmers(sequence, 30)\n",
    "    print(\"kmers built\")\n",
    "    file_to_hadoop=\"/Users/brzoskpm/Karl/genomes/kmer\"+filename\n",
    "    kmers.to_csv(file_to_hadoop, index=False)\n",
    "    print(\"write kmers to local file\",file_to_hadoop )\n",
    "    #publisher.write(kmers, f'kmersChr{i}.csv')\n",
    "    subprocess.call(f'hdfs dfs -copyFromLocal {file_to_hadoop}',shell=True)\n",
    "    print(\"written to hdfs\")\n",
    "    i=i+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13666e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmers.to_csv(\"/Users/brzoskpm/Karl/genomes/test\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b3ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark1 = SparkSession.builder.master(\"local\").appName(\"hdfs_test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f90c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=spark.read.csv(\"hdfs://localhost:9000/user/brzoskpm/kmerchr22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af5e258",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmerCount = data.groupBy(\"_c0\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33a0a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS WHERE MONGODB HAPPENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf92b1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/Cellar/apache-spark/3.3.0/libexec/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/karlbrzoska/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/karlbrzoska/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-19662785-ca1b-4296-8eb8-34df8597cacb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;2.4.2 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.12.5 in central\n",
      ":: resolution report :: resolve 167ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#mongo-java-driver;3.12.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;2.4.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-19662785-ca1b-4296-8eb8-34df8597cacb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/7ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/13 23:55:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "appName = \"PySpark MongoDB Examples\"\n",
    "master = \"local\"\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(appName) \\\n",
    "    .master(master) \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/BigDataManagementProject.Kmers\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/BigDataManagementProject.Kmers\") \\\n",
    "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:2.4.2')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad6fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = spark.read.csv('/Users/karlbrzoska/Documents/genomes/kmerchrY.count.csv').toDF('Kmer', 'Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f0cb470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "T.write.format(\"mongo\").option('uri', 'mongodb://127.0.0.1')\\\n",
    "    .option('database', 'BigDataManagementProject') \\\n",
    "    .option('collection', 'ChrY') \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146931f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmerCount.write.format(\"mongo\").option('uri', 'mongodb://127.0.0.1')\\\n",
    "    .option('database', 'BigDataManagementProject') \\\n",
    "    .option('collection', 'ChrY') \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
